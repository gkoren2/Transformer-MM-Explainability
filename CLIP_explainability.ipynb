{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiplefhuIUDd"
      },
      "source": [
        "# **CLIP Explainability**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ogYpvQAAH4s",
        "outputId": "9fff6b83-2551-4644-d59b-bc627535ac97"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/hila-chefer/Transformer-MM-Explainability\n",
        "\n",
        "# import os\n",
        "# os.chdir(f'./Transformer-MM-Explainability')\n",
        "\n",
        "# !pip install einops\n",
        "# !pip install ftfy\n",
        "# !pip install captum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8sl0DTeHuKx"
      },
      "source": [
        "# **CLIP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or8UETbZAYY3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import CLIP.clip as clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from captum.attr import visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DhG24G9cTpHT"
      },
      "outputs": [],
      "source": [
        "#@title Control context expansion (number of attention layers to consider)\n",
        "#@title Number of layers for image Transformer\n",
        "start_layer =  -1#@param {type:\"number\"}\n",
        "\n",
        "#@title Number of layers for text Transformer\n",
        "start_layer_text =  -1#@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWKGyu2YAeSV"
      },
      "outputs": [],
      "source": [
        "def interpret(image, texts, model, device, start_layer=start_layer, start_layer_text=start_layer_text):\n",
        "    batch_size = texts.shape[0]\n",
        "    images = image.repeat(batch_size, 1, 1, 1)\n",
        "    logits_per_image, logits_per_text = model(images, texts)\n",
        "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
        "    index = [i for i in range(batch_size)]\n",
        "    one_hot = np.zeros((logits_per_image.shape[0], logits_per_image.shape[1]), dtype=np.float32)\n",
        "    one_hot[torch.arange(logits_per_image.shape[0]), index] = 1\n",
        "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "    one_hot = torch.sum(one_hot.cuda() * logits_per_image)\n",
        "    model.zero_grad()\n",
        "\n",
        "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
        "\n",
        "    if start_layer == -1: \n",
        "      # calculate index of last layer \n",
        "      start_layer = len(image_attn_blocks) - 1\n",
        "    \n",
        "    num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
        "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
        "    R = R.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
        "    for i, blk in enumerate(image_attn_blocks):\n",
        "        if i < start_layer:\n",
        "          continue\n",
        "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
        "        cam = blk.attn_probs.detach()\n",
        "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "        cam = grad * cam\n",
        "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
        "        cam = cam.clamp(min=0).mean(dim=1)\n",
        "        R = R + torch.bmm(cam, R)\n",
        "    image_relevance = R[:, 0, 1:]\n",
        "\n",
        "    \n",
        "    text_attn_blocks = list(dict(model.transformer.resblocks.named_children()).values())\n",
        "\n",
        "    if start_layer_text == -1: \n",
        "      # calculate index of last layer \n",
        "      start_layer_text = len(text_attn_blocks) - 1\n",
        "\n",
        "    num_tokens = text_attn_blocks[0].attn_probs.shape[-1]\n",
        "    R_text = torch.eye(num_tokens, num_tokens, dtype=text_attn_blocks[0].attn_probs.dtype).to(device)\n",
        "    R_text = R_text.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
        "    for i, blk in enumerate(text_attn_blocks):\n",
        "        if i < start_layer_text:\n",
        "          continue\n",
        "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
        "        cam = blk.attn_probs.detach()\n",
        "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "        cam = grad * cam\n",
        "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
        "        cam = cam.clamp(min=0).mean(dim=1)\n",
        "        R_text = R_text + torch.bmm(cam, R_text)\n",
        "    text_relevance = R_text\n",
        "   \n",
        "    return text_relevance, image_relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MJ-Ech7dj6C"
      },
      "outputs": [],
      "source": [
        "def show_image_relevance(image_relevance, image, orig_image):\n",
        "    # create heatmap from mask on image\n",
        "    def show_cam_on_image(img, mask):\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "        heatmap = np.float32(heatmap) / 255\n",
        "        cam = heatmap + np.float32(img)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2)\n",
        "    axs[0].imshow(orig_image);\n",
        "    axs[0].axis('off');\n",
        "\n",
        "    dim = int(image_relevance.numel() ** 0.5)\n",
        "    image_relevance = image_relevance.reshape(1, 1, dim, dim)\n",
        "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
        "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
        "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
        "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    vis = show_cam_on_image(image, image_relevance)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "    axs[1].imshow(vis);\n",
        "    axs[1].axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsNrdWXOxub1"
      },
      "outputs": [],
      "source": [
        "from CLIP.clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "_tokenizer = _Tokenizer()\n",
        "\n",
        "def show_heatmap_on_text(text, text_encoding, R_text):\n",
        "  CLS_idx = text_encoding.argmax(dim=-1)\n",
        "  R_text = R_text[CLS_idx, 1:CLS_idx]\n",
        "  text_scores = R_text / R_text.sum()\n",
        "  text_scores = text_scores.flatten()\n",
        "  print(text_scores)\n",
        "  text_tokens=_tokenizer.encode(text)\n",
        "  text_tokens_decoded=[_tokenizer.decode([a]) for a in text_tokens]\n",
        "  vis_data_records = [visualization.VisualizationDataRecord(text_scores,0,0,0,0,0,text_tokens_decoded,1)]\n",
        "  visualization.visualize_text(vis_data_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YYjztv3Nn9V"
      },
      "outputs": [],
      "source": [
        "clip.clip._MODELS = {\n",
        "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
        "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
        "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pns9sG9eAhho",
        "outputId": "180c0886-cb3f-4554-82e2-4a51250f6692"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hIrp94ktMyc"
      },
      "outputs": [],
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "WoeuTHtyBq7Q",
        "outputId": "9d35d3ed-a997-4f23-e5d8-d4110efd4d7d"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/glasses.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"a man with eyeglasses\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "# R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device,\n",
        "                            start_layer=-1,start_layer_text=-1)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "2abGIlwSBeqf",
        "outputId": "3de25cfc-31aa-4054-85d5-ee0785adf017"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/lipstick.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "# texts = [\"a man with lipstick\"]\n",
        "texts = [\"a woman with thin eyebrows\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "4q4rMRkQCxzN",
        "outputId": "8bd73e32-1d4e-4764-d003-2d2f095b56f3"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/rocket.jpg\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"a rocket standing on a launchpad\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "fHn4tvHKAsHt",
        "outputId": "a2193482-003a-4bcf-9f89-4d8ac528659d"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/el2.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = ['a zebra', 'an elephant', 'a lake']\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "QXoI89YgAyem",
        "outputId": "e4ebd460-3d8d-48a1-af36-a84106d5603d"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/el3.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = ['a zebra', 'an elephant', 'water']\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "JrsCaea2HTtp",
        "outputId": "c389614a-13ce-4cc9-a433-71f2e0625308"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/el4.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = ['an elephant', 'a zebra']\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "WLTySnxDHdgu",
        "outputId": "bef4583e-e45a-4909-d02f-1671d3382c2c"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/el1.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = ['an elephant', 'a zebra']\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "d40nFlTCuUop",
        "outputId": "4da65d9d-0c12-4eea-c5ca-2ddbc2c8ea15"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/catdog.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"a dog\", \"a cat\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "a6B8WIaQuW8i",
        "outputId": "818387d7-a1ab-4299-dce0-f168061b7662"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/dogcat2.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"a labrador\", \"a tabby cat\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "L_MfDibFHhaL",
        "outputId": "967a326e-45ee-411b-8c50-71ce78521cf6"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/dogbird.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"a parrot\", \"a dog\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "RdfamDHPCerb",
        "outputId": "fe34c53a-cfa7-498f-f0fe-a3339c7b0222"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/astronaut.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"a portrait of an astronaut\", \"a rocket\", \"an astronaut suit\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7eC5rYE4WkS"
      },
      "source": [
        "# Examples of biased similarity scores (noise scores higher than dog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "EWiBw3aPzAY_",
        "outputId": "ff776e1a-0c65-405b-b12b-22269ea9f6ca"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/dog.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"an image of a dog\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "logits_per_image, logits_per_text = model(img, text)\n",
        "print(color.BOLD + color.PURPLE + color.UNDERLINE + f'CLIP similarity score: {logits_per_image.item()}' + color.END)\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "zhYtSI5tzKVN",
        "outputId": "3643a046-e28e-4b70-dc4a-b091444e12e5"
      },
      "outputs": [],
      "source": [
        "img_path = \"CLIP/noise.png\"\n",
        "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"an image of a dog\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "logits_per_image, logits_per_text = model(img, text)\n",
        "print(color.BOLD + color.PURPLE + color.UNDERLINE + f'CLIP similarity score: {logits_per_image.item()}' + color.END)\n",
        "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
        "batch_size = text.shape[0]\n",
        "for i in range(batch_size):\n",
        "  show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
        "  show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "CLIP-explainability.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
